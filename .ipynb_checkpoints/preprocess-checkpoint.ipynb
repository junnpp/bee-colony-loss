{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9040641c-ccea-459c-a509-8e337c3491c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de958cb-c186-4e4e-8b0b-968a2ec56537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http get requests / parse it into beautifulsoup\n",
    "\n",
    "# requests.adapters.DEFAULT_RETRIES = 5\n",
    "# increase retries number\n",
    "# s = requests.session()\n",
    "# s.keep_alive = False\n",
    "s = requests.get(\"https://usda.library.cornell.edu/concern/publications/rn301137d?locale=en\")\n",
    "html = s.text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69796dbd-96c2-4762-89c8-fd0e231320c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release-items > tr:nth-child(1) > td.file_set > div > a:nth-child(3)\n",
    "files = soup.select(\n",
    "    'tr > td > div > a'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa7781a-2c62-49c4-aff8-451d9c7a97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the download urls\n",
    "all_zip_urls = []\n",
    "for file in files:\n",
    "    if 'zip' in file.get('href'):\n",
    "        all_zip_urls.append(file.get('href'))\n",
    "        \n",
    "# remove redundant data: data recorded in 2022 is in both release and the lastest release section\n",
    "all_zip_urls = all_zip_urls[:-1]\n",
    "        \n",
    "all_zip_urls.sort(key = lambda x: x.split(\".\")[-2][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9d2ebe-7546-47c6-a5f4-7b2cc0a8ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data file name from the original data name\n",
    "dir_names = []\n",
    "\n",
    "for file_name in all_zip_urls:\n",
    "    \n",
    "    year = file_name.split('/')[-1].split(\".\")[0][-4:]\n",
    "        \n",
    "    if year.startswith(\"0\"):\n",
    "        file_name = \"20\" + year[-2:]\n",
    "        dir_names.append(file_name)\n",
    "    else:\n",
    "        file_name = year\n",
    "        dir_names.append(file_name)\n",
    "        \n",
    "dir_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e71acf4-fc03-4451-af0d-6e84e74d6731",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/55kbcz693ns4f2c487jlxqj00000gn/T/ipykernel_34688/2081585534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download each zip file into each data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# create a directory that will contain all raw dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_zip_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'dataset'"
     ]
    }
   ],
   "source": [
    "# download each zip file into each data directory\n",
    "# create a directory that will contain all raw dataset\n",
    "os.mkdir('dataset')\n",
    "\n",
    "for idx in range(len(all_zip_urls)):\n",
    "    \n",
    "    # sending the request to download zip files\n",
    "    r = requests.get(all_zip_urls[idx])\n",
    "    \n",
    "    # download each zip file into corresponding directory\n",
    "    name = os.path.join('dataset', dir_names[idx])\n",
    "    \n",
    "    # if the directory does not exist, create one\n",
    "    if not os.path.exists(name):\n",
    "        os.mkdir(name)\n",
    "        \n",
    "    # download each zip file with the name of the year\n",
    "    file_name = os.path.join('dataset', dir_names[idx], dir_names[idx])\n",
    "    \n",
    "    # writing bytes file - for zip files, you have to use 'wb'\n",
    "    # notice type(r.content) is bytes, not string\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413f932-b606-42f6-a4d0-697d65a55c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all zip files\n",
    "for idx in range(len(dir_names)):\n",
    "    zpath = os.path.join('dataset', dir_names[idx], dir_names[idx]) \n",
    "    \n",
    "    with zipfile.ZipFile(zpath, 'r') as z:\n",
    "        z.extractall(os.path.join('dataset', dir_names[idx]))\n",
    "\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a29c99-3d0a-4339-a6d4-4b426471326c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1) Colony Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0db24-5a9d-4151-81a7-4e9b892c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_colony(path):\n",
    "    # read and clean the data\n",
    "    col_labels = ['states', 'colony_n', 'colony_max', 'colony_lost', 'colony_lost_pct', 'colony_added', 'colony_reno', 'colony_reno_pct']\n",
    "    \n",
    "    # temporarily read the csv file and extract month and year of the current dataset\n",
    "    temp_df = pd.read_csv(path, encoding = 'Latin 1', engine = 'python', na_values = \"(Z)\", nrows = 1, header = 1)\n",
    "    month_year = list(set(re.findall(r\"\\d{4}|[\\w]+-[\\w]+\", temp_df.columns[2])))\n",
    "    month_year.sort(key = lambda x: len(x))\n",
    "    year = int(month_year[0])\n",
    "    month = month_year[1]\n",
    "\n",
    "    # read the given dataset skipping certain rows and columns to get the actual data\n",
    "    # put year and month columns\n",
    "    df = pd.read_csv(path, encoding = 'Latin 1', skiprows = 8, skipfooter = 11, engine = 'python', usecols = list(range(2, 10)), header =  0, names = col_labels, na_values = ['(Z)', '(NA)'])\n",
    "    df['year'] = year\n",
    "    df['month'] = month\n",
    "    df = df[df.columns[-2:].tolist() + df.columns[:-2].tolist()]\n",
    "    \n",
    "    # remove rows in which state value is null\n",
    "    # this indicates that the whole row consists of null\n",
    "    s = df.states.isnull()\n",
    "    index_to_drop = s.where(s==True).dropna().index\n",
    "    df = df.drop(index_to_drop)\n",
    "    \n",
    "    # from the dataset description, '-' indicates 0\n",
    "    df = df.replace(\"-\", 0)\n",
    "    \n",
    "    # filter out all the values that are not appropriate states names, including 'Other States.'\n",
    "    states = df.states.str.findall(r'[A-Za-z]+ [A-Za-z]+|[A-Za-z]+').apply(lambda x:x[0])\n",
    "    df.states = states\n",
    "    \n",
    "    # Lastly, convert data type of each column correspondingly\n",
    "    df = df.astype({\"year\": str, \"colony_n\": float, \"colony_max\": float, \"colony_lost\": float, \"colony_lost_pct\": float, \"colony_added\": float, \"colony_reno\": float, \"colony_reno_pct\": float})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04f649-2ead-435f-90e7-d4090409538e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize a list to contain all data paths\n",
    "whole_path = []\n",
    "year_path = [os.path.join('dataset', file) for file in dir_names]\n",
    "\n",
    "# for each year directory, only read files that contain colony data\n",
    "# order the files so that all of them are in order of time\n",
    "for cur_year in year_path:\n",
    "    cur_list = os.listdir(cur_year)\n",
    "    temp_list = []\n",
    "    \n",
    "    for file in cur_list:\n",
    "        match = re.search(r\"t005|t001|t007|t008|t011|t022\", file)\n",
    "    \n",
    "        if match:\n",
    "            temp_list.append(os.path.join(cur_year, file))\n",
    "            \n",
    "\n",
    "    temp_list.sort(key = lambda x: x.split(\"_\")[1])\n",
    "    whole_path = whole_path + temp_list\n",
    "    \n",
    "# combine those colony datasets into one\n",
    "colony = pd.concat(map(clean_colony, whole_path))\n",
    "\n",
    "# set the index number correctly\n",
    "idx = list(range(len(colony)))\n",
    "colony.index = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1ef84-bec6-496a-b3c2-1ea109a54984",
   "metadata": {},
   "outputs": [],
   "source": [
    "colony.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a91fc5-9bc2-41f4-8f12-f1b1784d8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "colony.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228dbea-d9a3-43e0-9550-9ced781bf11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colony.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece93119-c074-4c8d-b3e9-d704ce074c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colony.to_csv('colony.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c002ff7-3def-468c-a1a1-d81c0d7bb97c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2) Stressor Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f22ac7-bdc5-41c2-9f2f-d1ee78938e7f",
   "metadata": {},
   "source": [
    "Import stressor datasets by repeating the process of importing colony datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf7c6d-e528-4106-a45a-e9fd9b02cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stressor(path):\n",
    "    # read and clean the data\n",
    "    col_labels = ['states', 'Varroa Mites', 'Other pests/parasites', 'Diseases', 'Pesticides', 'Other', 'Unknown']\n",
    "    \n",
    "    # temporarily read the csv file and extract month and year of the current dataset\n",
    "    temp_df = pd.read_csv(path, encoding = 'Latin 1', engine = 'python', na_values = \"(Z)\", nrows = 1, header = 1)\n",
    "    month_year = list(set(re.findall(r\"\\d{4}|[\\w]+-[\\w]+\", temp_df.columns[2])))\n",
    "    month_year.sort(key = lambda x: len(x))\n",
    "    year = month_year[0]\n",
    "    month = month_year[1]\n",
    "\n",
    "    # read the given dataset skipping certain rows and columns to get the actual data\n",
    "    # put year and month columns\n",
    "    df = pd.read_csv(path, encoding = 'Latin 1', skiprows = 8, skipfooter = 8, usecols = list(range(2, 9)), engine = 'python', na_values = ['(Z)', '(NA)'], names = col_labels)\n",
    "    df['year'] = year\n",
    "    df['month'] = month\n",
    "    df = df[df.columns[-2:].tolist() + df.columns[:-2].tolist()]\n",
    "    \n",
    "    # remove rows in which state value is null\n",
    "    # this indicates that the whole row consists of null\n",
    "    s = df.states.isnull()\n",
    "    index_to_drop = s.where(s==True).dropna().index\n",
    "    df = df.drop(index_to_drop)\n",
    "    \n",
    "    # from the dataset description, '-' indicates 0\n",
    "    df = df.replace(\"-\", 0)\n",
    "    \n",
    "    # pivot the table: from wide to long\n",
    "    df = pd.melt(df, id_vars = ['year', 'month', 'states'], value_vars = ['Varroa Mites', 'Other pests/parasites', 'Diseases', 'Pesticides', 'Other', 'Unknown'], var_name = 'stressor', value_name = 'stress_pct').sort_values(by = 'states')\n",
    "    \n",
    "    # filter out all the values that are not appropriate states names, including 'Other States.'\n",
    "    states = df.states.str.findall(r'[A-Za-z]+ [A-Za-z]+|[A-Za-z]+').apply(lambda x:x[0])\n",
    "    df.states = states\n",
    "    \n",
    "    \n",
    "    # Lastly, convert data type of each column correspondingly\n",
    "    df = df.astype({'year': str, 'month': str, 'states': str, 'stressor': str, 'stress_pct': float})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92d60d-9b52-4c4e-85be-f18a7e48360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list to contain all data paths\n",
    "whole_path = []\n",
    "year_path = [os.path.join('dataset', file) for file in dir_names]\n",
    "\n",
    "# for each year directory, only read files that contain colony data\n",
    "# order the files so that all of them are in order of time\n",
    "for cur_year in year_path:\n",
    "    cur_list = os.listdir(cur_year)\n",
    "    temp_list = []\n",
    "    \n",
    "    for file in cur_list:\n",
    "        match = re.search(r\"t002|t013|t009|t010|t012|t023\", file)\n",
    "    \n",
    "        if match:\n",
    "            temp_list.append(os.path.join(cur_year, file))\n",
    "            \n",
    "\n",
    "    temp_list.sort(key = lambda x: x.split(\"_\")[1])\n",
    "    whole_path = whole_path + temp_list\n",
    "    \n",
    "\n",
    "# combine those colony datasets into one\n",
    "stressor = pd.concat(map(clean_stressor, whole_path))\n",
    "\n",
    "# set the index number correctly\n",
    "idx = list(range(len(stressor)))\n",
    "stressor.index = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc203a-277d-481d-b44d-5f04c2ea25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stressor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b22b7-5cbe-4f47-8619-18b345bf4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stressor.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc04695-2973-44ba-bf1a-bb603e6fad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "stressor.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a486822-422e-45cc-b4cc-95d98048cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stressor.to_csv(\"stressor.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
